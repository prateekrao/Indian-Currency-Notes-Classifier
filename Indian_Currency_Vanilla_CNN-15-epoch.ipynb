{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Indian-Currency-Vanilla-CNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN6HRgMAzsngprXSLL8Dkrq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/r-prateek/Indian-Currency-Notes-Classifier/blob/main/Indian_Currency_Vanilla_CNN-15-epoch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV7-Lm9Dlcmv"
      },
      "source": [
        "#**Importing Dataset from Kaggle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQXmRG6BlhnK"
      },
      "source": [
        "! pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets list\n",
        "! kaggle datasets download -d vishalmane109/indian-currency-note-images-dataset-2020\n",
        "! unzip indian-currency-note-images-dataset-2020.zip -d dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwTtN458mb2p"
      },
      "source": [
        "#**Using Image Generator for Data Augmentation and initializing paths for the datasets.**\n",
        "\n",
        "Libraries used are:\n",
        "```\n",
        "tensorflow\n",
        "matplotlib\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vahQLSzomzeN"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsNN8tgWm5d5"
      },
      "source": [
        "train_path = \"/content/dataset/Indian currency dataset v1/training\"\n",
        "test_path = \"/content/dataset/Indian currency dataset v1/validation\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGe2YaBLnTDZ"
      },
      "source": [
        "##Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmGR0tSVnXNl"
      },
      "source": [
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    horizontal_flip = True,\n",
        "    vertical_flip = True,\n",
        "    rotation_range= 5\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caGEDkprn5RQ",
        "outputId": "9a484f59-8b9e-47ec-f97d-4c60b919c645"
      },
      "source": [
        "train_data = datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size = (190, 190),\n",
        "    color_mode = 'rgb',\n",
        "    batch_size = 32,\n",
        "    shuffle = True,\n",
        "    seed = 865,\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3566 images belonging to 8 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDIfgId2ovw4",
        "outputId": "ca412431-5e78-4c73-a6f3-9ee4ab254d52"
      },
      "source": [
        "test_data = datagen.flow_from_directory(\n",
        "    test_path,\n",
        "    target_size = (190, 190),\n",
        "    color_mode = 'rgb',\n",
        "    batch_size = 32,\n",
        "    shuffle = True,\n",
        "    seed = 865\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 345 images belonging to 8 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11sd3YStpJWm"
      },
      "source": [
        "#**Creating the Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV3NyNHdpUTB"
      },
      "source": [
        "##Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAQHBaWdpdMU"
      },
      "source": [
        "The convolutional neural network plays an important role in feature extraction from the input image, sufficiently changing the weights (filter)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsfKM0hNpu1H"
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(32, (3,3), activation = 'relu', input_shape = (190, 190, 3)))\n",
        "model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(64, (3,3), activation= 'relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(64, (3,3), activation= 'relu'))\n",
        "# model.add(tf.keras.layers.MaxPooling2D((2,2)))\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH1Rrp5XwALH"
      },
      "source": [
        "##Artificial Deep Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuIFvYptwOOs"
      },
      "source": [
        "The Artificial Deep Neural Network flattens the input fed by the Convolutional network, and classifies images into respective classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlQOdDa4whLi"
      },
      "source": [
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "model.add(tf.keras.layers.Dense(32, activation= 'relu'))\n",
        "model.add(tf.keras.layers.Dense(64, activation= 'relu'))\n",
        "model.add(tf.keras.layers.Dense(8, activation= 'softmax'))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQz6IP5exReB"
      },
      "source": [
        "##Compiling and Summarising the model\n",
        "\n",
        "*  Optimizer used is **Adam**\n",
        "*  Loss function used is **Categorical CrossEntropy**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQtGzPJ8xhUD"
      },
      "source": [
        "model.compile(optimizer= 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgeG0ghTxqf3",
        "outputId": "caa4735b-5c34-4c8d-c85f-b8c85d34f4bd"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 188, 188, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 94, 94, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 92, 92, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 46, 46, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 44, 44, 64)        36928     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 44, 44, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 123904)            0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                3964960   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 8)                 520       \n",
            "=================================================================\n",
            "Total params: 4,023,912\n",
            "Trainable params: 4,023,912\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnPmUjhex44U"
      },
      "source": [
        "#**Training the Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUd_2l-Tx-aR"
      },
      "source": [
        "The Sequential model is trained on the augmented training dataset, and the validation data set will be the augmented test dataset. The model will be trained for **15 epochs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpefjsWWyR_J",
        "outputId": "9c4ae02f-6293-4f4e-d0f2-c8fc0ff638ed"
      },
      "source": [
        "history = model.fit(train_data, validation_data= test_data, epochs = 15, batch_size = 2000)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "112/112 [==============================] - ETA: 0s - loss: 2.1055 - accuracy: 0.1373"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "112/112 [==============================] - 307s 3s/step - loss: 2.1051 - accuracy: 0.1375 - val_loss: 1.8474 - val_accuracy: 0.2203\n",
            "Epoch 2/15\n",
            "112/112 [==============================] - 298s 3s/step - loss: 1.8694 - accuracy: 0.2506 - val_loss: 1.6483 - val_accuracy: 0.3391\n",
            "Epoch 3/15\n",
            "112/112 [==============================] - 295s 3s/step - loss: 1.6182 - accuracy: 0.3961 - val_loss: 1.3193 - val_accuracy: 0.5449\n",
            "Epoch 4/15\n",
            "112/112 [==============================] - 296s 3s/step - loss: 1.3544 - accuracy: 0.5012 - val_loss: 1.3087 - val_accuracy: 0.5420\n",
            "Epoch 5/15\n",
            "112/112 [==============================] - 300s 3s/step - loss: 1.2377 - accuracy: 0.5508 - val_loss: 1.2224 - val_accuracy: 0.5362\n",
            "Epoch 6/15\n",
            "112/112 [==============================] - 301s 3s/step - loss: 1.1414 - accuracy: 0.5578 - val_loss: 1.0181 - val_accuracy: 0.6464\n",
            "Epoch 7/15\n",
            "112/112 [==============================] - 301s 3s/step - loss: 0.9653 - accuracy: 0.6589 - val_loss: 0.9929 - val_accuracy: 0.6609\n",
            "Epoch 8/15\n",
            "112/112 [==============================] - 302s 3s/step - loss: 0.9477 - accuracy: 0.6671 - val_loss: 0.9791 - val_accuracy: 0.6319\n",
            "Epoch 9/15\n",
            "112/112 [==============================] - 299s 3s/step - loss: 0.8999 - accuracy: 0.6738 - val_loss: 0.8743 - val_accuracy: 0.7159\n",
            "Epoch 10/15\n",
            "112/112 [==============================] - 296s 3s/step - loss: 0.7986 - accuracy: 0.7291 - val_loss: 0.8655 - val_accuracy: 0.7072\n",
            "Epoch 11/15\n",
            "112/112 [==============================] - 296s 3s/step - loss: 0.7459 - accuracy: 0.7296 - val_loss: 0.8632 - val_accuracy: 0.7217\n",
            "Epoch 12/15\n",
            "112/112 [==============================] - 297s 3s/step - loss: 0.6755 - accuracy: 0.7640 - val_loss: 0.8421 - val_accuracy: 0.7420\n",
            "Epoch 13/15\n",
            "112/112 [==============================] - 295s 3s/step - loss: 0.6400 - accuracy: 0.7742 - val_loss: 0.8356 - val_accuracy: 0.7536\n",
            "Epoch 14/15\n",
            "112/112 [==============================] - 297s 3s/step - loss: 0.5773 - accuracy: 0.7970 - val_loss: 0.7690 - val_accuracy: 0.7710\n",
            "Epoch 15/15\n",
            "112/112 [==============================] - 301s 3s/step - loss: 0.5420 - accuracy: 0.8108 - val_loss: 0.7748 - val_accuracy: 0.7855\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}